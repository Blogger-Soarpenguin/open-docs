---
redirect_from:
  - /learn/run-hadoop-on-mesos-using-installer/
layout: tutorial
title: Run Apache Hadoop on Apache Mesos using the Mesosphere Installer
tutorial_icon_classname: tutorial-icon-hadoop
framework: hadoop
tutorial_name: run-hadoop-on-mesos-using-installer
overview: Introduction and prerequisites for getting Hadoop running on Mesos
steps:
  - title: Launch your Elastic Mesos Cluster
    blurb: >
      Launch a Mesos cluster either using the Getting Started with Apache Mesos tutorial or through the Elastic Mesos web service
    icon: fa-rocket
    blackboard: >
      <span data-img-placeholder
        data-alt=""
        data-class="thumbnail img-responsive"
        data-src="{% asset_path learn/hadoop/hadoop.png %}"
        itemprop="image"></span>
    instruction: >
      <p data-personalized="hide">First, launch a Mesosphere cluster. See our <a target="_blank" href="/getting-started/">Getting Started documentation</a> for instructions on setting up a cluster on a variety of platforms.
      </p>
      <p>Hadoop can take advantage of the elasticity and fault tolerance Mesos provides. Running Hadoop on top of Mesos lets you share your cluster resources with other frameworks to yield higher utilization.</p>
      <p>Mesos is made up of a set of master nodes and a set of slaves. For this tutorial, we're running the Hadoop <a href="http://wiki.apache.org/hadoop/JobTracker">JobTracker</a> on one of the master nodes and the TaskTrackers will be launched on slave nodes. We will demonstrate how to configure Hadoop to leverage Mesos directly.</p><br />
      <p>This tutorial uses the Hadoop installer which performs lots of magic to make it much simpler to install Hadoop on Mesos.</p>
  - title: Check out the Mesos Web UI
    blurb: >
      Verify your Mesos cluster is up and running by navigating to the Web UI
    icon: fa-globe
    blackboard: >
      <img src="{% asset_path learn/hadoop.00.mesos_console.png %}" title="Mesos console" alt="" class="thumbnail img-responsive"/>
    instruction: >
      <p>Now, verify your Mesos cluster is up and running by navigating to the Web UI at <code>http://<span data-replace="master-ip">54.211.52.241</span>:5050</code></p>
      <p>In this example, we have a cluster of six nodes running three Mesos masters and three Mesos slaves. We highly recommend running the 18-node cluster for Hadoop but for very basic jobs, a 6 node installation will work as well. Now verify that you have at least 10GB of ram and 5 cpus available for the Hadoop framework.</p>
  - title: Connect to the Mesos master
    blurb: >
      Use SSH to log into one of the Mesos master nodes
    icon: fa-exchange
    blackboard: >
      <pre class="terminal"><code>bash-3.2$ ssh -l ubuntu <span data-replace="master-ip">54.211.52.241</span>

      Welcome to Ubuntu 12.10 (GNU/Linux 3.5.0-41-generic x86_64)

       * Documentation:  https://help.ubuntu.com/

        System information as of Fri Nov  1 18:52:24 UTC 2013

        System load:  0.0               Processes:           74

        Usage of /:   16.1% of 7.87GB   Users logged in:     0

        Memory usage: 3%                IP address for eth0: <span data-replace="master-private-ip">10.145.227.194</span>

        Swap usage:   0%

      Last login: Fri Nov  1 17:54:10 2013 from 107-202-144-131.example.com</code></pre>
    instruction: >
      <p>Now let's use <code>ssh</code> to login to the Mesos master at <code data-replace="master-ip">54.226.218.168</code> to get Apache Hadoop installed: <pre data-code class="clear"><code class="codeblock">ssh -l ubuntu <span data-replace="master-ip">54.226.218.168</span></code></pre></p>
      <p>For Elastic Mesos, we've chosen Ubuntu as the Linux distribution, and we've added the public key that you specified during the launch with this account.</p>
  - title: Download the Hadoop installer scripts
    blurb: >
      Download and prepare the Hadoop installer script from Amazon AWS S3
    icon: fa-cloud-download
    blackboard: >
      <pre class="terminal"><code>ubuntu:~$ wget https://s3.amazonaws.com/downloads.mesosphere.io/dcc/apps/hadoop/bin/medium_mr1-2.0.0-mr1-cdh4.2.1

      --2013-11-01 18:57:37--  https://s3.amazonaws.com/downloads.mesosphere.io/dcc/apps/hadoop/bin/medium_mr1-2.0.0-mr1-cdh4.2.1

      Resolving s3.amazonaws.com (s3.amazonaws.com)... 176.32.100.65

      Connecting to s3.amazonaws.com (s3.amazonaws.com)|176.32.100.65|:443... connected.

      HTTP request sent, awaiting response... 200 OK

      Length: 1399 (1.4K) [binary/octet-stream]

      Saving to: `medium_mr1-2.0.0-mr1-cdh4.2.1'


      100%[=====================================================================================>] 1,399       --.-K/s   in 0s


      2013-11-01 18:57:38 (336 MB/s) - `medium_mr1-2.0.0-mr1-cdh4.2.1' saved [1399/1399]


      ubuntu:~$ chmod +x medium_mr1-2.0.0-mr1-cdh4.2.1</code></pre>
    instruction: >
      <p>Next, use <code>wget</code> to download the Hadoop installer script from Amazon AWS S3: <pre data-code class="clear"><code class="codeblock">wget https://s3.amazonaws.com/downloads.mesosphere.io/dcc/apps/hadoop/bin/medium_mr1-2.0.0-mr1-cdh4.2.1</code></pre></p>
      <p>Then use <code>chmod</code> to make the script executable: <pre data-code class="clear"><code class="codeblock">chmod +x medium_mr1-2.0.0-mr1-cdh4.2.1</code></pre></p>
  - title: Run the Hadoop installer scripts
    blurb: >
      The installer scripts prepares the cluster for running Hadoop with Mesos. This step involves setting network configurations and repackaging Cloudera’s CDH 4.2.1 MR1 distribution
    icon: fa-cog
    blackboard: >
      <pre class="terminal"><code>ubuntu:~$ PORT0=8811 PORT1=8812 JAVA_HOME=/usr ./medium_mr1-2.0.0-mr1-cdh4.2.1

      + curl -sSfL https://downloads.mesosphere.io/dcc/apps/hadoop/bin/hadoop-launcher --output hadoop-launcher

      + bash -x ./hadoop-launcher https://downloads.mesosphere.io/dcc/apps/hadoop/distros/mr1-2.0.0-mr1-cdh4.2.1.tar.gz --conf-uris https://downloads.mesosphere.io/dcc/apps/hadoop/conf/conf-cdh4.tgz --lib-uris https://s3.amazonaws.com/mesosphere-maven-public/org/apache/mesos/mesos_jdk6/0.14.0/mesos_jdk6-0.14.0.jar https://downloads.mesosphere.io/dcc/apps/hadoop/hadoop-mesos/hadoop-mesos-0.0.3.jar --replace 'mapred-child-java-opts=-XX:+UseParallelGC -Xmx768m' mapred-tasktracker-map-tasks-maximum=30 mapred-tasktracker-reduce-tasks-maximum=30 mapreduce-tasktracker-http-threads=25 mapreduce-compress=true mapreduce-compress-codec=org.apache.hadoop.io.compress.SnappyCodec mapreduce-task-io-sort-mb=100 mapred-attempts=6 mapred-jobtracker-taskScheduler=org.apache.hadoop.mapred.MesosScheduler mapred-mesos-taskScheduler=org.apache.hadoop.mapred.FairScheduler mapreduce-task-io-sort-factor=35 mapred-mesos-slot-cpus=0.85 mapred-mesos-slot-disk=512 mapred-mesos-slot-mem=512 mapred-mesos-tasktracker-mem=1024 mapred-mesos-total-map-slots-minimum=4 mapred-mesos-total-reduce-slots-minimum=4 mapred-mesos-tasktracker-cpus=0.75


      ...</code></pre>
    instruction: >
      <p>Next we need to specify a couple unused, high-numbered ports for Hadoop UIs. With the <code>JAVA_HOME</code>, <code>PORT0</code>, and <code>PORT1</code> environment variables set, we run the script to install and launch Hadoop: <pre data-code class="clear"><code class="codeblock">PORT0=8811 PORT1=8812 JAVA_HOME=/usr ./medium_mr1-2.0.0-mr1-cdh4.2.1</code></pre></p>
      <p>This command does several things. First, it sets the RPC port to <code>8811</code>, the JobTracker web UI port to <code>8812</code>, and the <code>JAVA_HOME</code> to the <b>/usr</b> directory. The latter is the default location for Java on Ubuntu -- change as needed on other Linux distros.</p>
      <p>In addition, this commands installs and configures Hadoop so that it can run on Mesos. Note that we're using the Cloudera <b>CDH 4.2.1-MR1</b> Hadoop distro. Also, we modify its configuration and add a Mesos/Hadoop bridge in the <b>lib</b> directory.</p>
      <p>Finally, we repackage the modified Hadoop distro and load it into HDFS so that the Mesos slaves can access it.</p>
      </p>Please note: Hadoop runs in the foreground with this configuration, we will leave it running and later login with another SSH session. If you were to use this in production, you would likely want to start Hadoop either via <em>init.d</em>, <em>upstart</em> or <em><a href="http://github.com/mesosphere/marathon">Marathon</a></em></p>
      <p><em>Caveat: if you were using this installer script <b>not</b> on Elastic Mesos, the script would need some modifications.</em></p>
  - title: Make sure the task trackers are running
    blurb: >
      Eventually, the output should show that the task trackers have launched
    icon: fa-search
    blackboard: >
      <pre class="terminal"><code>...


      13/11/01 19:01:54 INFO mapred.MesosScheduler: Status update of Task_Tracker_0 to TASK_RUNNING with message

      13/11/01 19:01:54 INFO net.NetworkTopology: Adding a new node: /default-rack/ec2-107-22-33-3.compute-1.amazonaws.com

      13/11/01 19:01:54 INFO mapred.JobTracker: Adding tracker tracker_ec2-107-22-33-3.compute-1.amazonaws.com:ec2-107-22-33-3.compute-1.amazonaws.com/10.100.211.24:31001 to host ec2-107-22-33-3.compute-1.amazonaws.com</code></pre>
    instruction: >
      <p>Eventually, the output should show that the task trackers have launched.</p>
  - title: Hadoop can now be seen in the Web UI
    blurb: >
      In the Mesos console in the browser, notice that Hadoop is running as one of the active frameworks
    icon: fa-globe
    blackboard: >
      <img src="{% asset_path learn/hadoop.01.active_frameworks.png %}" alt="" class="thumbnail img-responsive">
    instruction: >
      <p>Back to the Mesos console in the browser, notice that Hadoop is running as one of the active frameworks.</p>
      <p>Click on the "ID" link for the Hadoop framework, which should look something like <code>…787-0000</code></p>
  - title: Select a task tracker
    blurb: >
      Click on the link for one of the task trackers listed under "Active Tasks" to view its sandbox. It will contain the files associated with the process
    icon: fa-folder-open-o
    blackboard: >
      <img src="{% asset_path learn/hadoop.02.hadoop_fw.png %}" alt="" class="thumbnail img-responsive">
    instruction: >
      <p>Next, click on the <b>Sandbox</b> link for one of the task trackers listed under "Active Tasks" to view the files associated with that process: <b>stdout</b>, <b>stderr</b>, etc.</p>
  - title: Open the stderr file to view the task tracker output
    blurb: >
      Click on the stderr link to open a window that shows the stderrlog. The updates continue to append to this log while it is open
    icon: fa-file-o
    blackboard: >
      <img src="{% asset_path learn/hadoop.03.hadoop_sandbox.png %}" alt="" class="thumbnail img-responsive">
    instruction: >
      <p>Now click on the <b>stderr</b> link to open a window that shows the <b>stderr</b> log. Note that updates continue to append to this log while it is open.</p>
  - title: Take a look at the task tracker output with the log viewer
    blurb: >
      Examining the stderr log is a good way to verify that your Hadoop cluster has launched correctly
    icon: fa-sort-alpha-desc
    blackboard: >
      <img src="{% asset_path learn/hadoop.04.hadoop_stderr.png %}" alt="" class="thumbnail img-responsive">
    instruction: >
      <p>Examining the <b>stderr</b> log is a good way to verify that your Hadoop cluster has launched correctly. There should be no errors getting appended as you watch its updates.</p>
  - title: Connect to the Mesos master
    blurb: >
      Use SSH to log into one of the Mesos master nodes as in step 3
    icon: fa-exchange
    blackboard: >
      <pre class="terminal"><code>bash-3.2$ ssh -l ubuntu <span data-replace="master-ip">54.211.52.241<span>

      Welcome to Ubuntu 12.10 (GNU/Linux 3.5.0-41-generic x86_64)

       * Documentation:  https://help.ubuntu.com/

        System information as of Fri Nov  1 19:15:18 UTC 2013

        System load:  0.0               Processes:           82

        Usage of /:   24.4% of 7.87GB   Users logged in:     1

        Memory usage: 15%               IP address for eth0: 10.145.227.194

        Swap usage:   0%

      Last login: Fri Nov  1 18:52:25 2013 from 107-202-144-131.example.com</code></pre>
    instruction: >
      <p>Close the window that shows the <b>stderr</b> log. Now let's cut over to running Hadoop jobs from a command line, using <code>ssh</code> to login to the Mesos master again -- from a <b>different</b> terminal window: <pre data-code class="clear"><code class="codeblock">ssh -l ubuntu <span data-replace="master-ip">54.226.218.168</span></code></pre></p>
  - title: Switch user and setup the Hadoop executable path
    blurb: >
      Hadoop requires that we change to the mapred user and also, we must make sure to use the proper Hadoop executable
    icon: fa-user
    blackboard: >
      <pre class="terminal"><code>ubuntu:~$ sudo -u mapred bash

      mapred:~$ export HADOOP_HOME=/tmp/hadoop ; ls $HADOOP_HOME

      bin          cloudera-pom.xml  hadoop-ant-2.0.0-mr1-cdh4.2.1.jar       ivy          mapred      src

      build.xml    conf              hadoop-core-2.0.0-mr1-cdh4.2.1.jar      ivy.xml      NOTICE.txt  webapps

      c++          contrib           hadoop-examples-2.0.0-mr1-cdh4.2.1.jar  lib          pids

      CHANGES.txt  docs              hadoop-test-2.0.0-mr1-cdh4.2.1.jar      LICENSE.txt  README.txt

      cloudera     example-confs     hadoop-tools-2.0.0-mr1-cdh4.2.1.jar     logs         sbin</code></pre>
    instruction: >
      <p>Okay, next up... Hadoop requires that we change to the <code>mapred</code> user: <pre data-code class="clear"><code class="codeblock">sudo -u mapred bash</code></pre></p>
      <p>Also, we must make sure to use the proper Hadoop executable -- the one configured to use Mesos. That's located in the <code>/tmp/hadoop</code> directory, so set the <code>HADOOP_HOME</code> environment variable to point to it: <pre data-code class="clear"><code class="codeblock">export HADOOP_HOME=/tmp/hadoop ; ls $HADOOP_HOME</code></pre></p>
  - title: Get ready to run the pre-built sample app
    blurb: >
      Apache Hadoop ships with a pre-built sample app, WordCount. Write some example data to two files
    icon: fa-pencil
    blackboard: >
      <pre class="terminal"><code>mapred:~$ echo "Hello World Bye World" > /tmp/file0

      mapred:~$ echo "Hello Hadoop Goodbye Hadoop" > /tmp/file1</code></pre>
    instruction: >
      <p>Fortunately, Apache Hadoop ships with a pre-built sample app, the ubiquitous <b>WordCount</b> example. That example needs data, so let's create two files of input to use with it. The first data file is: <pre data-code class="clear"><code class="codeblock">echo "Hello World Bye World" > /tmp/file0</code></pre></p>
      <p>The second data file is: <pre data-code class="clear"><code class="codeblock">echo "Hello Hadoop Goodbye Hadoop" > /tmp/file1</code></pre></p>
  - title: Upload the example data to HDFS
    blurb: >
      Create a directory on HDFS and copy the data files into the newly created HDFS directory
    icon: fa-cloud-upload
    blackboard: >
      <pre class="terminal"><code>mapred:~$ /tmp/hadoop/bin/hadoop fs -mkdir /user/foo/data

      mapred:~$ /tmp/hadoop/bin/hadoop fs -copyFromLocal /tmp/file? /user/foo/data</code></pre>
    instruction: >
      <p>Next, create a directory on HDFS using <code>foo</code> as a user name: <pre data-code class="clear"><code class="codeblock">/tmp/hadoop/bin/hadoop fs -mkdir /user/foo/data</code></pre></p>
      <p>Then copy the data files into the newly created HDFS directory: <pre data-code class="clear"><code class="codeblock">/tmp/hadoop/bin/hadoop fs -copyFromLocal /tmp/file? /user/foo/data</code></pre></p>
  - title: Run WordCount
    blurb: >
      Everything is ready to submit the JAR file for the WordCount app to Hadoop. Let's run that puppy now
    icon: fa-play
    blackboard: >
      <pre class="terminal"><code>mapred:~$ /tmp/hadoop/bin/hadoop jar /tmp/hadoop/hadoop-examples-2.0.0-mr1-cdh4.2.1.jar wordcount /user/foo/data /user/foo/out

      13/11/01 19:31:07 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.

      13/11/01 19:31:07 INFO input.FileInputFormat: Total input paths to process : 2

      13/11/01 19:31:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

      13/11/01 19:31:08 INFO mapred.JobClient: Running job: job_201311011901_0001

      13/11/01 19:31:09 INFO mapred.JobClient:  map 0% reduce 0%

      13/11/01 19:31:23 INFO mapred.JobClient:  map 50% reduce 0%

      13/11/01 19:31:24 INFO mapred.JobClient:  map 100% reduce 0%

      13/11/01 19:31:27 INFO mapred.JobClient: Job complete: job_201311011901_0001

      13/11/01 19:31:27 INFO mapred.JobClient: Counters: 32

      13/11/01 19:31:27 INFO mapred.JobClient:   File System Counters

      13/11/01 19:31:27 INFO mapred.JobClient:     FILE: Number of bytes read=79

      13/11/01 19:31:27 INFO mapred.JobClient:     FILE: Number of bytes written=1216377

      13/11/01 19:31:27 INFO mapred.JobClient:     FILE: Number of read operations=0

      13/11/01 19:31:27 INFO mapred.JobClient:     FILE: Number of large read operations=0

      13/11/01 19:31:27 INFO mapred.JobClient:     FILE: Number of write operations=0

      13/11/01 19:31:27 INFO mapred.JobClient:     HDFS: Number of bytes read=263

      13/11/01 19:31:27 INFO mapred.JobClient:     HDFS: Number of bytes written=49

      13/11/01 19:31:27 INFO mapred.JobClient:     HDFS: Number of read operations=4

      13/11/01 19:31:27 INFO mapred.JobClient:     HDFS: Number of large read operations=0

      13/11/01 19:31:27 INFO mapred.JobClient:     HDFS: Number of write operations=2

      13/11/01 19:31:27 INFO mapred.JobClient:   Job Counters

      13/11/01 19:31:27 INFO mapred.JobClient:     Launched map tasks=2

      13/11/01 19:31:27 INFO mapred.JobClient:     Launched reduce tasks=1

      13/11/01 19:31:27 INFO mapred.JobClient:     Data-local map tasks=2

      13/11/01 19:31:27 INFO mapred.JobClient:     Total time spent by all maps in occupied slots (ms)=8322

      13/11/01 19:31:27 INFO mapred.JobClient:     Total time spent by all reduces in occupied slots (ms)=6391

      13/11/01 19:31:27 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0

      13/11/01 19:31:27 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0

      13/11/01 19:31:27 INFO mapred.JobClient:   Map-Reduce Framework

      13/11/01 19:31:27 INFO mapred.JobClient:     Map input records=2

      13/11/01 19:31:27 INFO mapred.JobClient:     Map output records=8

      13/11/01 19:31:27 INFO mapred.JobClient:     Map output bytes=82

      13/11/01 19:31:27 INFO mapred.JobClient:     Input split bytes=212

      13/11/01 19:31:27 INFO mapred.JobClient:     Combine input records=8

      13/11/01 19:31:27 INFO mapred.JobClient:     Combine output records=6

      13/11/01 19:31:27 INFO mapred.JobClient:     Reduce input groups=5

      13/11/01 19:31:27 INFO mapred.JobClient:     Reduce shuffle bytes=101

      13/11/01 19:31:27 INFO mapred.JobClient:     Reduce input records=6

      13/11/01 19:31:27 INFO mapred.JobClient:     Reduce output records=5

      13/11/01 19:31:27 INFO mapred.JobClient:     Spilled Records=12

      13/11/01 19:31:27 INFO mapred.JobClient:     CPU time spent (ms)=2600

      13/11/01 19:31:27 INFO mapred.JobClient:     Physical memory (bytes) snapshot=583737344

      13/11/01 19:31:27 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=5261574144

      13/11/01 19:31:27 INFO mapred.JobClient:     Total committed heap usage (bytes)=642318336</code></pre>
    instruction: >
      <p>Okay, everything is ready to submit the JAR file for the <b>WordCount</b> app to Hadoop. Let's run that puppy now: <pre data-code class="clear"><code class="codeblock">/tmp/hadoop/bin/hadoop jar /tmp/hadoop/hadoop-examples-2.0.0-mr1-cdh4.2.1.jar wordcount /user/foo/data /user/foo/out</code></pre></p>
  - title: Download and view the result from HDFS
    blurb: >
      Check the output of the Hadoop job. You should see some word counts. Let's now wish ourselves a heartfelt "Hooah!"
    icon: fa-cloud-download
    blackboard: >
      <pre class="terminal"><code>mapred:~$ /tmp/hadoop/bin/hadoop fs -ls /user/foo/out

      Found 2 items

      drwxr-xr-x   - mapred hadoop          0 2013-11-01 19:31 /user/foo/out/_logs

      -rw-r--r--   3 mapred hadoop         49 2013-11-01 19:31 /user/foo/out/part-r-00000.snappy


      mapred:~$ $HADOOP_HOME/bin/hadoop dfs -cat /user/foo/out/part*

      Bye 1

      Goodbye 1

      Hadoop 2

      Hello 2

      World 2</code></pre>
    instruction: >
      <p>Great, let's check the output of that Hadoop job. List the directory in HDFS: <pre data-code class="clear"><code class="codeblock">$HADOOP_HOME/bin/hadoop fs -ls /user/foo/out</code></pre></p>
      <p>All's well that ends well. BTW, don't worry about runtime warnings; Hadoop has an overabundance of those. Otherwise, look at the results: <pre data-code class="clear"><code class="codeblock">$HADOOP_HOME/bin/hadoop dfs -cat /user/foo/out/part*</code></pre></p>
      <p>You should see some word counts. Let's now wish ourselves a heartfelt "Hooah!"</p>
      <p>Of course, in practice you'll probabaly have much more sophisticated use cases, and much larger and more complex data sets. In any case, those usages will follow the same general pattern as this one. You Hadooper, you.</p>
success:
  title: Next steps
  blurb: >
    A slightly more detailed description for what happens in this step of the tutorial
  text: >
    <p>Congratulations! You now have Hadoop running on Mesos!</p>
---
