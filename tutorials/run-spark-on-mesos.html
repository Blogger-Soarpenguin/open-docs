---
redirect_from:
  - /learn/run-spark-on-mesos/
layout: tutorial
title: Run Apache Spark on Apache Mesos
tutorial_icon_classname: tutorial-icon-spark
framework: spark
tutorial_name: run-spark-on-mesos
overview: Introduction and prerequisites for getting Spark running on Mesos
steps:
  - title: Launch a Mesos cluster
    blurb: >
      Launch a Mesos cluster and load the Mesos console in a browser
    icon: fa-rocket
    blackboard: >
      <span data-img-placeholder
        data-alt=""
        data-class="thumbnail img-responsive"
        data-src="{% asset_path learn/hadoop.00.mesos_console.png %}"
        itemprop="image"></span>
    instruction: >
      <p data-personalized="hide">First, launch a Mesosphere cluster. See our <a target="_blank" href="/getting-started/">Getting Started documentation</a> for instructions on setting up a cluster on a variety of platforms. Then load the Mesos console in a browser window.</p>
      <p>The Mesos console at <code>http://<span data-replace="master-ip">54.226.218.168</span>:5050</code> shows that we have a cluster of six nodes running three Mesos masters and three Mesos slaves.</p>
  - title: Login to the Mesos Master
    blurb: >
      Use ssh to login to the Mesos master and install Apache Spark
    icon: fa-terminal
    blackboard: >
      <pre class="terminal"><code>bash-3.2$ ssh -l ubuntu <span data-replace="master-ip">54.226.218.168</span>

      Welcome to Ubuntu 12.10 (GNU/Linux 3.5.0-41-generic x86_64)

       * Documentation:  https://help.ubuntu.com/

        System information as of Sat Nov  9 00:24:13 UTC 2013

        System load:  0.0               Processes:           74

        Usage of /:   16.1% of 7.87GB   Users logged in:     0

        Memory usage: 2%                IP address for eth0: 10.34.165.84

        Swap usage:   0%

      Last login: Sat Nov  9 00:11:02 2013 from 107-202-144-131.example.com</code></pre>
    instruction: >
      <p>Now let's use <code>ssh</code> to login to the Mesos master at <code data-replace="master-ip">54.226.218.168</code> to get Apache Spark installed: <pre data-code class="clear"><code class="codeblock">ssh -l ubuntu <span data-replace="master-ip">54.226.218.168</span></code></pre></p>
      <p>For Elastic Mesos, we've chosen Ubuntu as the Linux distribution, and we've added the public key that you specified during the launch with this account.</p>
  - title: Install Git
    blurb: >
      Use aptitude to install Git, which gets used later in the Spark installation
    icon: fa-wrench
    blackboard: >
      <pre class="terminal"><code>ubuntu:~$ sudo aptitude install git

      The following NEW packages will be installed:

        git git-man{a} liberror-perl{a}

      0 packages upgraded, 3 newly installed, 0 to remove and 29 not upgraded.

      Need to get 6,824 kB of archives. After unpacking 15.3 MB will be used.

      Do you want to continue? [Y/n/?] Y

      Get: 1 http://us-east-1.ec2.archive.ubuntu.com/ubuntu/ quantal/main liberror-perl all 0.17-1 [23.8 kB]

      Get: 2 http://us-east-1.ec2.archive.ubuntu.com/ubuntu/ quantal/main git-man all 1:1.7.10.4-1ubuntu1 [634 kB]

      Get: 3 http://us-east-1.ec2.archive.ubuntu.com/ubuntu/ quantal/main git amd64 1:1.7.10.4-1ubuntu1 [6,165 kB]

      Fetched 6,824 kB in 0s (14.8 MB/s)

      Selecting previously unselected package liberror-perl.

      (Reading database ... 52841 files and directories currently installed.)

      Unpacking liberror-perl (from .../liberror-perl_0.17-1_all.deb) ...

      Selecting previously unselected package git-man.

      Unpacking git-man (from .../git-man_1%3a1.7.10.4-1ubuntu1_all.deb) ...

      Selecting previously unselected package git.

      Unpacking git (from .../git_1%3a1.7.10.4-1ubuntu1_amd64.deb) ...

      Processing triggers for man-db ...

      Setting up liberror-perl (0.17-1) ...

      Setting up git-man (1:1.7.10.4-1ubuntu1) ...

      Setting up git (1:1.7.10.4-1ubuntu1) ...</code></pre>
    instruction: >
      <p>Next, we use <code>aptitude</code> to install <a href="http://git-scm.com/" target="_blank">Git</a>, which gets used later in the Spark installation: <pre data-code class="clear"><code class="codeblock">sudo aptitude install git</code></pre></p>
  - title: Install Java JDK 7
    blurb: >
      Use aptitude to install Java JDK 7 which is required by Spark
    icon: fa-wrench
    blackboard: >
      <pre class="terminal"><code>ubuntu:~$ sudo aptitude -y install openjdk-7-jdk

      The following NEW packages will be installed:

        acl{a} at-spi2-core{a} colord{a} cpp{a} cpp-4.7{a} dbus-x11{a} dconf-gsettings-backend{a} dconf-service{a} fontconfig{a}
        gconf-service{a} gconf-service-backend{a} gconf2{a} gconf2-common{a} gvfs{a} gvfs-common{a} gvfs-daemons{a} gvfs-libs{a}
        hicolor-icon-theme{a} libasound2{a} libasyncns0{a} libatasmart4{a} libatk-bridge2.0-0{a} libatk-wrapper-java{a}
        libatk-wrapper-java-jni{a} libatk1.0-0{a} libatk1.0-data{a} libatspi2.0-0{a} libavahi-glib1{a} libbonobo2-0{a}
        libbonobo2-common{a} libcairo-gobject2{a} libcairo2{a} libcanberra0{a} libcolord1{a} libdatrie1{a} libdconf1{a}
        libdrm-nouveau2{a} libexif12{a} libflac8{a} libfontenc1{a} libgconf-2-4{a} libgconf2-4{a} libgd2-xpm{a}
        libgdk-pixbuf2.0-0{a} libgdk-pixbuf2.0-common{a} libgif4{a} libgl1-mesa-dri{a} libgl1-mesa-glx{a} libglapi-mesa{a}
        libgnome2-0{a} libgnome2-bin{a} libgnome2-common{a} libgnomevfs2-0{a} libgnomevfs2-common{a} libgphoto2-2{a}
        libgphoto2-l10n{a} libgphoto2-port0{a} libgtk-3-0{a} libgtk-3-bin{a} libgtk-3-common{a} libgtk2.0-0{a} libgtk2.0-bin{a}
        libgtk2.0-common{a} libgusb2{a} libice-dev{a} libice6{a} libidl-common{a} libidl0{a} libieee1284-3{a} libjasper1{a}
        libjbig0{a} libjson0{a} libllvm3.1{a} libltdl7{a} libmpc2{a} libmpfr4{a} libogg0{a} liborbit2{a} libpango1.0-0{a}
        libpixman-1-0{a} libpthread-stubs0{a} libpthread-stubs0-dev{a} libpulse0{a} libsane{a} libsane-common{a}
        libsecret-1-0{a} libsecret-common{a} libsm-dev{a} libsm6{a} libsndfile1{a} libtdb1{a} libthai-data{a} libthai0{a}
        libtiff5{a} libtxc-dxtn-s2tc0{a} libudisks2-0{a} libv4l-0{a} libv4lconvert0{a} libvorbis0a{a} libvorbisenc2{a}
        libvorbisfile3{a} libx11-dev{a} libx11-doc{a} libx11-xcb1{a} libxau-dev{a} libxaw7{a} libxcb-glx0{a} libxcb-render0{a}
        libxcb-shape0{a} libxcb-shm0{a} libxcb1-dev{a} libxcomposite1{a} libxcursor1{a} libxdamage1{a} libxdmcp-dev{a}
        libxfixes3{a} libxft2{a} libxi6{a} libxinerama1{a} libxmu6{a} libxpm4{a} libxrandr2{a} libxrender1{a} libxt-dev{a}
        libxt6{a} libxtst6{a} libxv1{a} libxxf86dga1{a} libxxf86vm1{a} mtools{a} openjdk-7-jdk openjdk-7-jre{a}
        policykit-1-gnome{a} shared-mime-info{a} sound-theme-freedesktop{a} ttf-dejavu-extra{a} udisks2{a} x11-common{a}
        x11-utils{a} x11proto-core-dev{a} x11proto-input-dev{a} x11proto-kb-dev{a} xorg-sgml-doctools{a} xtrans-dev{a}

      0 packages upgraded, 144 newly installed, 0 to remove and 29 not upgraded.

      Need to get 61.6 MB of archives. After unpacking 165 MB will be used.

      ...</code></pre>
    instruction: >
      <p>Then we use <code>aptitude</code> to install <a href="http://openjdk.java.net/" target="_blank">Java JDK 7</a>, which is required by Spark: <pre data-code class="clear"><code class="codeblock">sudo aptitude -y install openjdk-7-jdk</code></pre></p>
  - title: Download Spark
    blurb: >
      Download and inflate the Spark tarball for the latest stable version
    icon: fa-cloud-download
    blackboard: >
      <pre class="terminal"><code>ubuntu:~$ wget https://d3kbcqa49mib13.cloudfront.net/spark-0.8.0-incubating.tgz

      --2013-11-09 00:29:49--  http://d3kbcqa49mib13.cloudfront.net/spark-0.8.0-incubating.tgz

      Resolving spark-project.org (spark-project.org)... 128.32.37.248

      Connecting to spark-project.org (spark-project.org)|128.32.37.248|:80... connected.

      HTTP request sent, awaiting response... 307 Temporary Redirect

      Location: http://d3kbcqa49mib13.cloudfront.net/spark-0.8.0-incubating.tgz [following]

      --2013-11-09 00:29:50--  http://d3kbcqa49mib13.cloudfront.net/spark-0.8.0-incubating.tgz

      Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 54.240.160.99, 54.230.17.89, 54.230.17.120, ...

      Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|54.240.160.99|:80... connected.

      HTTP request sent, awaiting response... 200 OK

      Length: 140612059 (134M) [application/x-compressed]

      Saving to: `spark-0.8.0-incubating.tgz'


      100%[=====================================================================================>] 140,612,059 15.1M/s   in 8.4s


      2013-11-09 00:29:58 (15.9 MB/s) - `spark-0.8.0-incubating.tgz' saved [140612059/140612059]


      ubuntu:~$ tar xzf spark-0.8.0-incubating.tgz

      ubuntu:~$ cd spark-0.8.0-incubating/</code></pre>

    instruction: >
      <p>Now we're ready to download Spark, based on the instructions given on its <a href="http://spark.incubator.apache.org/documentation.html" target="_blank">documentation page</a>.</p>
      <p>Use <code>wget</code> to pull down the tarball for the stable version, which is <b>0.8.0</b> in this case: <pre data-code class="clear"><code class="codeblock">wget https://d3kbcqa49mib13.cloudfront.net/spark-0.8.0-incubating.tgz</code></pre></p>
      <p>Then inflate the tarball: <pre data-code class="clear"><code class="codeblock">tar xzf spark-0.8.0-incubating.tgz</code></pre></p>
      <p>Next, connect into the newly created directory for the Spark distribution: <pre data-code class="clear"><code class="codeblock">cd spark-0.8.0-incubating/</code></pre></p>
  - title: Build Spark
    blurb: >
      Build Spark using the sbt build tool for Scala
    icon: fa-coffee
    blackboard: >
      <pre class="terminal"><code>ubuntu:~/spark-0.8.0-incubating-bin$ SPARK_HADOOP_VERSION=2.0.0-mr1-cdh4.4.0 sbt/sbt clean assembly

      Getting net.java.dev.jna jna 3.2.3 ...

      downloading http://repo1.maven.org/maven2/net/java/dev/jna/jna/3.2.3/jna-3.2.3.jar ...

          [SUCCESSFUL ] net.java.dev.jna#jna;3.2.3!jna.jar (113ms)

      :: retrieving :: org.scala-sbt#boot-jna

          confs: [default]

          1 artifacts copied, 0 already retrieved (838kB/39ms)

      Getting org.scala-sbt sbt 0.12.4 ...



      [info] Checking every *.class/*.jar file's SHA-1.

      [info] SHA-1: 1e99bae998cb96697f95cf8d8b44370b6a7bae71

      [info] Packaging /home/ubuntu/spark-0.8.0-incubating/examples/target/scala-2.9.3/spark-examples-assembly-0.8.0-incubating.jar ...

      [info] Done packaging.

      [success] Total time: 806 s, completed Nov 9, 2013 12:46:08 AM</code></pre>

    instruction: >
      <p>At this point, we're ready to build Spark using the <a href="http://www.scala-sbt.org/" target="_blank">sbt</a> build tool for Scala. Note that we must specify the Hadoop version through the <code>SPARK_HADOOP_VERSION</code> environment variable: <pre data-code class="clear"><code class="codeblock">SPARK_HADOOP_VERSION=2.0.0-mr1-cdh4.4.0 sbt/sbt clean assembly</code></pre></p>
      <p>Spark may take several minutes to build. There will most likely be several warnings -- not to worry. For example, you can safely ignore this one: <pre><code>cp: cannot stat `/home/ubuntu/spark-0.8.0-incubating/conf/*.template': No such file or directory</code></pre></p>
      <p>Remember: Keep calm and Use isolation.</p>
  - title: Package Spark distribution
    blurb: >
      Package your newly built Spark distribution for the Mesos slaves to use
    icon: fa-compress
    blackboard: >
      <pre class="terminal"><code>ubuntu:~/spark-0.8.0-incubating$ ./make-distribution.sh --hadoop 2.0.0-mr1-cdh4.4.0

      ...

      [success] Total time: 69 s, completed Nov 9, 2013 12:55:03 AM


      ubuntu:~/spark-0.8.0-incubating-bin$ mv dist spark-0.8.0-2.0.0-mr1-cdh4.4.0

      ubuntu:~/spark-0.8.0-incubating-bin$ tar czf spark-0.8.0-2.0.0-mr1-cdh4.4.0.tgz spark-0.8.0-2.0.0-mr1-cdh4.4.0</code></pre>

    instruction: >
      <p>Now we package our newly built Spark distribution for the Mesos slaves to use. First, run the <b>make-distribution.sh</b> script: <pre data-code class="clear"><code class="codeblock">./make-distribution.sh --hadoop 2.0.0-mr1-cdh4.4.0</code></pre></p>
      <p>Rename the <b>dist</b> directory which we just created to incorporate its version info for Hadoop and Spark: <pre data-code class="clear"><code class="codeblock">mv dist spark-0.8.0-2.0.0-mr1-cdh4.4.0</code></pre></p>
      <p>Then use <code>tar</code> to create a tarball for this distribution: <pre data-code class="clear"><code class="codeblock">tar czf spark-0.8.0-2.0.0-mr1-cdh4.4.0.tgz spark-0.8.0-2.0.0-mr1-cdh4.4.0</code></pre></p>
      <p>This may take several minutes to build, so hold tight. Also, if you plan on using Spark a lot, keep a copy of this tarball elsewhere so that you don't have to keep building it.</p>
  - title: Load Spark distribution into HDFS
    blurb: >
      Load the packaged distribution of Spark into HDFS so that it's accessible from Mesos
    icon: fa-upload
    blackboard: >
      <pre class="terminal"><code>ubuntu:~/spark-0.8.0-incubating-bin-cdh4$ hadoop fs -mkdir /tmp

      ubuntu:~/spark-0.8.0-incubating-bin$ hadoop fs -put spark-0.8.0-2.0.0-mr1-cdh4.4.0.tgz /tmp</code></pre>

    instruction: >
      <p>Now we must load the packaged distribution of Spark into <a href="http://spark.incubator.apache.org/docs/latest/running-on-mesos.html" target="_blank">HDFS</a> so that it's accessible from Mesos. First, create a <b>/tmp</b> directory in HDFS: <pre data-code class="clear"><code class="codeblock">hadoop fs -mkdir /tmp</code></pre></p>
      <p>Then copy the tarball into that newly created directory: <pre data-code class="clear"><code class="codeblock">hadoop fs -put spark-0.8.0-2.0.0-mr1-cdh4.4.0.tgz /tmp</code></pre></p>
  - title: Populate Spark Configuration
    blurb: >
      Populate the config file for the Spark runtime environment
    icon: fa-cog
    blackboard: >
      <pre class="terminal"><code>ubuntu:~/spark-0.8.0-incubating$ cd conf/

      ubuntu:~/spark-0.8.0-incubating/conf$ cp spark-env.sh.template spark-env.sh

      ubuntu:~/spark-0.8.0-incubating/conf$ vim spark-env.sh

      ubuntu:~/spark-0.8.0-incubating/conf$ cat spark-env.sh

      #!/usr/bin/env bash

      export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so

      export SPARK_EXECUTOR_URI=hdfs://<span data-replace="namenode">54.204.196.36</span>/tmp/spark-0.8.0-2.0.0-mr1-cdh4.4.0.tgz

      export MASTER=zk://<span data-replace="zookeeper">54.226.218.168:2181</span><span data-replace="zookeeper-path">/mesos</span>

      ubuntu:~/spark-0.8.0-incubating/conf$ cd ..</code></pre>

    instruction: >
      <p>The config file for the Spark runtime environment needs to be populated. First, connect into the <b>conf</b> directory: <pre data-code class="clear"><code class="codeblock">cd conf/</code></pre></p>
      <p>Then copy the template into the <b>spark-env.sh</b> file which we need to edit: <pre data-code class="clear"><code class="codeblock">cp spark-env.sh.template spark-env.sh</code></pre></p>
      <p>Use the text editor of your choice to modify the config file, in this case we use <code>vim</code>: <pre data-code class="clear"><code class="codeblock">vim spark-env.sh</code></pre></p>
      <p>Insert three lines into the config file. First, specify the location of the <b>libmesos.so</b> library. Then define the URI for the Spark executor, which the Mesos slaves will run. Finally, define the URIs for the Zookeeper masters: <pre data-code class="clear"><code class="codeblock">export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so

      export SPARK_EXECUTOR_URI=hdfs://<span data-replace="namenode">54.204.196.36</span>/tmp/spark-0.8.0-2.0.0-mr1-cdh4.4.0.tgz

      export MASTER=zk://<span data-replace="zookeeper">54.226.218.168:2181</span><span data-replace="zookeeper-path">/mesos</span></code></pre></p>
      <p>Save the file and confirm its contents: <pre data-code class="clear"><code class="codeblock">cat spark-env.sh</code></pre></p>
      <p>Then connect back into the parent directory: <pre data-code class="clear"><code class="codeblock">cd ..</code></pre></p>
  - title: Run Spark
    blurb: >
      Launch the Spark Shell
    icon: fa-play
    blackboard: >
      <pre class="terminal"><code>ubuntu:~/spark-0.8.0-incubating$ ./spark-shell

      Welcome to

            ____              __
           / __/__  ___ _____/ /__
          _\ \/ _ \/ _ `/ __/  '_/
         /___/ .__/\_,_/_/ /_/\_\   version 0.8.0
            /_/

      Using Scala version 2.9.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_25)

      Initializing interpreter...

      13/11/09 17:21:23 INFO server.Server: jetty-7.x.y-SNAPSHOT

      13/11/09 17:21:23 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:46678

      Creating SparkContext...

      13/11/09 17:21:33 INFO slf4j.Slf4jEventHandler: Slf4jEventHandler started

      13/11/09 17:21:33 INFO spark.SparkEnv: Registering BlockManagerMaster

      13/11/09 17:21:33 INFO storage.MemoryStore: MemoryStore started with capacity 323.9 MB.

      13/11/09 17:21:33 INFO storage.DiskStore: Created local directory at /tmp/spark-local-20131109172133-fea7

      13/11/09 17:21:33 INFO network.ConnectionManager: Bound socket to port 53657 with id = ConnectionManagerId(ec2-54-234-234-236.compute-1.amazonaws.com,53657)

      13/11/09 17:21:33 INFO storage.BlockManagerMaster: Trying to register BlockManager

      13/11/09 17:21:33 INFO storage.BlockManagerMaster: Registered BlockManager

      13/11/09 17:21:33 INFO server.Server: jetty-7.x.y-SNAPSHOT

      13/11/09 17:21:33 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:56990

      13/11/09 17:21:33 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.235.3.248:56990

      13/11/09 17:21:33 INFO spark.SparkEnv: Registering MapOutputTracker

      13/11/09 17:21:33 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-aadb6109-9113-4137-84d4-5c4a029c6d1a

      13/11/09 17:21:33 INFO server.Server: jetty-7.x.y-SNAPSHOT

      13/11/09 17:21:33 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:37205

      13/11/09 17:21:33 INFO server.Server: jetty-7.x.y-SNAPSHOT

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage/rdd,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/stage,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/pool,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/environment,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/executors,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}

      13/11/09 17:21:33 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/,null}

      13/11/09 17:21:33 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040

      13/11/09 17:21:33 INFO ui.SparkUI: Started Spark Web UI at http://ec2-54-234-234-236.compute-1.amazonaws.com:4040

      I1109 17:21:33.760414  8699 detector.cpp:234] Master detector (scheduler(1)@10.235.3.248:36864) connected to ZooKeeper ...

      I1109 17:21:33.760835  8699 detector.cpp:251] Trying to create path '<span data-replace="zookeeper-path">/mesos</span>' in ZooKeeper

      I1109 17:21:33.764375  8699 detector.cpp:420] Master detector (scheduler(1)@10.235.3.248:36864)  found 3 registered masters

      I1109 17:21:33.765884  8699 detector.cpp:467] Master detector (scheduler(1)@10.235.3.248:36864)  got new master pid: master@10.171.11.145:5050

      13/11/09 17:21:33 INFO mesos.MesosSchedulerBackend: Registered as framework ID 201311091642-2433461002-5050-821-0001

      Spark context available as sc.

      Type in expressions to have them evaluated.

      Type :help for more information.


      scala></code></pre>

    instruction: >
      <p>In the language of the Texans, "Wee haw!" We're ready to run Spark, so let's launch the REPL: <pre data-code class="clear"><code class="codeblock">./spark-shell</code></pre></p>
      <p>After that Spark REPL starts up correctly, you should have a <b>scala&gt;</b> prompt.</p>
  - title: Create a simple data set
    blurb: >
      Create a simple data set and distribute the collection as a Spark RDD
    icon: fa-tasks
    blackboard: >
      <pre class="terminal"><code>scala> val data = 1 to 10000

      data: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170...


      scala> val distData = sc.parallelize(data)

      distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:14


      scala></code></pre>

    instruction: >
      <p>Let's create a simple data set, a list of integers ranging from 1 to 10,000 as a Scala collection. At the REPL, type: <pre data-code class="clear"><code class="codeblock">val data = 1 to 10000</code></pre></p>
      <p>Then we distribute the collection -- to parallelize processing with it -- as a Spark <a href="http://spark.incubator.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell" target="_blank">RDD</a>: <pre data-code class="clear"><code class="codeblock">val distData = sc.parallelize(data)</code></pre></p>
  - title: Run a Spark job
    blurb: >
      Run a filter on your distributed data set and select only values less than 10
    icon: fa-trophy
    blackboard: >
      <pre class="terminal"><code>scala> distData.filter(_< 10).collect()

      13/11/09 17:22:35 INFO spark.SparkContext: Starting job: collect at <console>:17

      13/11/09 17:22:35 INFO scheduler.DAGScheduler: Got job 0 (collect at <console>:17) with 8 output partitions (allowLocal=false)

      13/11/09 17:22:35 INFO scheduler.DAGScheduler: Final stage: Stage 0 (collect at <console>:17)

      13/11/09 17:22:35 INFO scheduler.DAGScheduler: Parents of final stage: List()

      13/11/09 17:22:35 INFO scheduler.DAGScheduler: Missing parents: List()

      13/11/09 17:22:35 INFO scheduler.DAGScheduler: Submitting Stage 0 (FilteredRDD[1] at filter at <console>:17), which has no missing parents

      13/11/09 17:22:35 INFO scheduler.DAGScheduler: Submitting 8 missing tasks from Stage 0 (FilteredRDD[1] at filter at <console>:17)

      13/11/09 17:22:35 INFO cluster.ClusterScheduler: Adding task set 0.0 with 8 tasks

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Starting task 0.0:0 as TID 0 on executor 201311091642-2433461002-5050-821-0: ec2-54-211-111-148.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:0 as 1354 bytes in 7 ms

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Starting task 0.0:1 as TID 1 on executor 201311091642-2433461002-5050-821-2: ec2-54-204-237-232.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:1 as 1354 bytes in 0 ms

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Starting task 0.0:2 as TID 2 on executor 201311091642-2433461002-5050-821-1: ec2-54-234-38-234.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:2 as 1354 bytes in 1 ms

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Starting task 0.0:3 as TID 3 on executor 201311091642-2433461002-5050-821-0: ec2-54-211-111-148.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:3 as 1354 bytes in 0 ms

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Starting task 0.0:4 as TID 4 on executor 201311091642-2433461002-5050-821-2: ec2-54-204-237-232.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:4 as 1354 bytes in 0 ms

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Starting task 0.0:5 as TID 5 on executor 201311091642-2433461002-5050-821-1: ec2-54-234-38-234.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:35 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:5 as 1354 bytes in 1 ms

      13/11/09 17:22:50 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager ec2-54-211-111-148.compute-1.amazonaws.com:36446 with 323.9 MB RAM

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Finished TID 3 in 16004 ms on ec2-54-211-111-148.compute-1.amazonaws.com (progress: 1/8)

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Finished TID 0 in 16029 ms on ec2-54-211-111-148.compute-1.amazonaws.com (progress: 2/8)

      13/11/09 17:22:51 INFO scheduler.DAGScheduler: Completed ResultTask(0, 3)

      13/11/09 17:22:51 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0)

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Starting task 0.0:6 as TID 6 on executor 201311091642-2433461002-5050-821-0: ec2-54-211-111-148.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:6 as 1354 bytes in 0 ms

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Starting task 0.0:7 as TID 7 on executor 201311091642-2433461002-5050-821-0: ec2-54-211-111-148.compute-1.amazonaws.com (PROCESS_LOCAL)

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Serialized task 0.0:7 as 1354 bytes in 1 ms

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Finished TID 7 in 31 ms on ec2-54-211-111-148.compute-1.amazonaws.com (progress: 3/8)

      13/11/09 17:22:51 INFO scheduler.DAGScheduler: Completed ResultTask(0, 7)

      13/11/09 17:22:51 INFO cluster.ClusterTaskSetManager: Finished TID 6 in 42 ms on ec2-54-211-111-148.compute-1.amazonaws.com (progress: 4/8)

      13/11/09 17:22:51 INFO scheduler.DAGScheduler: Completed ResultTask(0, 6)

      13/11/09 17:22:55 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager ec2-54-204-237-232.compute-1.amazonaws.com:49041 with 323.9 MB RAM

      13/11/09 17:22:56 INFO cluster.ClusterTaskSetManager: Finished TID 1 in 20608 ms on ec2-54-204-237-232.compute-1.amazonaws.com (progress: 5/8)

      13/11/09 17:22:56 INFO scheduler.DAGScheduler: Completed ResultTask(0, 1)

      13/11/09 17:22:56 INFO cluster.ClusterTaskSetManager: Finished TID 4 in 20608 ms on ec2-54-204-237-232.compute-1.amazonaws.com (progress: 6/8)

      13/11/09 17:22:56 INFO scheduler.DAGScheduler: Completed ResultTask(0, 4)

      13/11/09 17:22:56 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager ec2-54-234-38-234.compute-1.amazonaws.com:59403 with 323.9 MB RAM

      13/11/09 17:22:56 INFO cluster.ClusterTaskSetManager: Finished TID 2 in 21298 ms on ec2-54-234-38-234.compute-1.amazonaws.com (progress: 7/8)

      13/11/09 17:22:56 INFO scheduler.DAGScheduler: Completed ResultTask(0, 2)

      13/11/09 17:22:56 INFO cluster.ClusterTaskSetManager: Finished TID 5 in 21296 ms on ec2-54-234-38-234.compute-1.amazonaws.com (progress: 8/8)

      13/11/09 17:22:56 INFO scheduler.DAGScheduler: Completed ResultTask(0, 5)

      13/11/09 17:22:56 INFO scheduler.DAGScheduler: Stage 0 (collect at <console>:17) finished in 12.322 s

      13/11/09 17:22:56 INFO cluster.ClusterScheduler: Remove TaskSet 0.0 from pool

      13/11/09 17:22:56 INFO spark.SparkContext: Job finished: collect at <console>:17, took 12.561347305 s

      res0: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)


      scala></code></pre>

    instruction: >
      <p>Now for some real work... Let's run a filter on this distributed data set <b>distData</b> and select only the values which are less than 10. At the REPL, type: <pre data-code class="clear"><code class="codeblock">distData.filter(_&lt; 10).collect()</code></pre></p>
      <p>Bokay, congrats on running a Spark job.</b>
  - title: Explore Spark console
    blurb: >
      Navigate to the Spark console in your browser and explore completed tasks
    icon: fa-list-alt
    blackboard: >
      <span data-img-placeholder
        data-alt=""
        data-class="thumbnail img-responsive"
        data-src="{% asset_path learn/spark.00.spark_stages.png %}"></span>
    instruction: >
      <p>Back in the browser, check out the Spark console. Its URL will be the Mesos master IP address on port <code>4040</code>, which is <code>http://<span data-replace="master-ip">54.226.218.168</span>:4040</code> in this case.</p>
      <p>Notice there have been 8 tasks that succeeded, and that took a little over 12 seconds to complete.</p>
  - title: Explore Mesos console
    blurb: >
      Navigate to the Mesos console in your browser and explore active frameworks
    icon: fa-list-alt
    blackboard: >
      <span data-img-placeholder
        data-alt=""
        data-class="thumbnail img-responsive"
        data-src="{% asset_path learn/spark.01.mesos_console.png %}"></span>
    instruction: >
      <p>Moving to the Mesos console in the browser -- its URL will be the Mesos master IP address on port <code>5050</code> -- which is <code>http://<span data-replace="master-ip">54.226.218.168</span>:5050</code> in this case.</p>
      <p>Notice that <b>Spark shell</b> is running as one of the active frameworks. Click on the "ID" link for the Spark framework, which should look something like <code>…821-0001</code></p>
  - title: View the Spark Framework within Mesos
    blurb: >
      View the completed tasks for the Spark Framework and select a completed task to explore
    icon: fa-list
    blackboard: >
      <span data-img-placeholder
        data-alt=""
        data-class="thumbnail img-responsive"
        data-src="{% asset_path learn/spark.02.completed_tasks.png %}"
        data-title="Mesos console"></span>
    instruction: >
      <p>Next, click on the <b>Sandbox</b> link for one of the Spark tasks listed under "Completed Tasks" to view the files associated with that process: <b>stdout</b>, <b>stderr</b>, etc.</p>
  - title: Explore a Task’s Sandbox
    blurb: >
      View the files associated with a completed Spark task and click on the stderr link to open a window which shows that log
    icon: fa-list
    blackboard: >
      <span data-img-placeholder
        data-alt=""
        data-class="thumbnail img-responsive"
        data-src="{% asset_path learn/spark.03.sandbox.png %}"
        data-title="Mesos console"></span>
    instruction: >
      <p>Click on the <b>stderr</b> link to open a window which shows that log.</p>
  - title: Verify Spark Task Ran Correctly
    blurb: >
      Look at the stderr log to verify that your Spark cluster launched and the task ran correctly
    icon: fa-file-text-o
    blackboard: >
      <span data-img-placeholder
        data-alt=""
        data-class="thumbnail img-responsive"
        data-src="{% asset_path learn/spark.04.stderr.png %}"
        data-title="Mesos console"></span>
    instruction: >
      <p>Looking at the <b>stderr</b> log is a good way to verify that your Spark cluster has launched and run correctly. There should be no errors getting appended as you watch the updates.</p>
success:
  title: Next steps
  blurb: >
    A slightly more detailed description for what happens in this step of the tutorial
  text: >
    <p>Congratulations! That's what it takes to run Spark on Mesos!</p>
---
